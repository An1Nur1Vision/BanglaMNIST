{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 8\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "def get_train_X_Y():\n",
    "    import scipy.io as spio\n",
    "    import numpy as np\n",
    "\n",
    "    mat = spio.loadmat('ISI_BN_Trn18000Tst4000(XBN).mat', squeeze_me=True)\n",
    "\n",
    "    org_train_x = mat['org_train_x']\n",
    "    org_train_y = mat['org_train_y']\n",
    "\n",
    "\n",
    "    train_X = np.empty((18000, 28, 28))\n",
    "    size=(48,48)\n",
    "    for i in range(0,18000):\n",
    "    #     print(i)\n",
    "        if i >= 3600 and i < 10800:\n",
    "            img = np.rot90(org_train_x[i].reshape(28,28),k =3)\n",
    "            img = np.fliplr(img)\n",
    "    #     elif i >= 9000 and i < 10800:\n",
    "    #         img = np.rot90(org_train_x[i].reshape(28,28),k =3)\n",
    "    #         img = np.fliplr(img)\n",
    "        elif i >= 12600 and i < 14400:\n",
    "            img = np.rot90(org_train_x[i].reshape(28,28),k =3)\n",
    "            img = np.fliplr(img)\n",
    "        elif i>= 14400 and i < 16200:\n",
    "            img = np.rot90(org_train_x[i].reshape(28,28),k =3)\n",
    "            img = np.fliplr(img)\n",
    "        else:\n",
    "            img = org_train_x[i].reshape(28,28)\n",
    "    #     height, width = img.shape[:2]\n",
    "    #     dst = cv2.resize(img, (5*width, 5*height), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    #     x = np.pad(img,pad_width=2, mode='constant', constant_values=[0])\n",
    "    #     cv2.imwrite(\"output_saved_mat/digit_\"+ str(i) +\".png\",x*255)\n",
    "\n",
    "        train_X[i] = img\n",
    "\n",
    "\n",
    "    zero = train_X[0:1800]\n",
    "    one = train_X[5400:7200]\n",
    "    two = train_X[3600:5400]\n",
    "    three = train_X[1800:3600]\n",
    "    four = train_X[7200:9000]\n",
    "    five = train_X[9000:10800]\n",
    "    six = train_X[16200:18000]\n",
    "    seven = train_X[12600:14400]\n",
    "    eight = train_X[14400:16200]\n",
    "    nine = train_X[10800:12600]\n",
    "\n",
    "    train_x = np.concatenate((zero, one, two, three, four, five, six, seven, eight, nine))\n",
    "\n",
    "    y = []\n",
    "    for j in range(10):\n",
    "        for i in range(1800):\n",
    "                y.append(j)\n",
    "\n",
    "    train_y = to_categorical(y)\n",
    "\n",
    "    return (train_x, train_y)\n",
    "#         return (train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f982c57048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f764bf87f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train = shuffle(get_train_X_Y()[0], get_train_X_Y()[1])\n",
    "\n",
    "# print(y_train)\n",
    "print(x_train.shape)\n",
    "\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (18000, 28, 28, 1)\n",
      "18000 train samples\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "# x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12059 samples, validate on 5941 samples\n",
      "Epoch 1/500\n",
      "12059/12059 [==============================] - 2s 193us/step - loss: 2.3025 - acc: 0.0990 - val_loss: 2.3023 - val_acc: 0.0971\n",
      "Epoch 2/500\n",
      "12059/12059 [==============================] - 1s 116us/step - loss: 2.3011 - acc: 0.1160 - val_loss: 2.3004 - val_acc: 0.0956\n",
      "Epoch 3/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 2.1445 - acc: 0.3046 - val_loss: 1.3311 - val_acc: 0.6459\n",
      "Epoch 4/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.7646 - acc: 0.7564 - val_loss: 0.4909 - val_acc: 0.8431\n",
      "Epoch 5/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.5298 - acc: 0.8289 - val_loss: 0.4017 - val_acc: 0.8638\n",
      "Epoch 6/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.4573 - acc: 0.8511 - val_loss: 0.3845 - val_acc: 0.8761\n",
      "Epoch 7/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.4117 - acc: 0.8667 - val_loss: 0.3021 - val_acc: 0.8998\n",
      "Epoch 8/500\n",
      "12059/12059 [==============================] - 1s 114us/step - loss: 0.3857 - acc: 0.8757 - val_loss: 0.2807 - val_acc: 0.9094\n",
      "Epoch 9/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.3575 - acc: 0.8842 - val_loss: 0.2736 - val_acc: 0.9145\n",
      "Epoch 10/500\n",
      "12059/12059 [==============================] - 1s 112us/step - loss: 0.3471 - acc: 0.8878 - val_loss: 0.2540 - val_acc: 0.9207\n",
      "Epoch 11/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.3292 - acc: 0.8927 - val_loss: 0.2360 - val_acc: 0.9219\n",
      "Epoch 12/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.3226 - acc: 0.8945 - val_loss: 0.2360 - val_acc: 0.9231\n",
      "Epoch 13/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.3065 - acc: 0.8987 - val_loss: 0.2218 - val_acc: 0.9285\n",
      "Epoch 14/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.2887 - acc: 0.9051 - val_loss: 0.2269 - val_acc: 0.9295\n",
      "Epoch 15/500\n",
      "12059/12059 [==============================] - 1s 114us/step - loss: 0.2899 - acc: 0.9039 - val_loss: 0.2084 - val_acc: 0.9323\n",
      "Epoch 16/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.2842 - acc: 0.9057 - val_loss: 0.2098 - val_acc: 0.9344\n",
      "Epoch 17/500\n",
      "12059/12059 [==============================] - 1s 110us/step - loss: 0.2838 - acc: 0.9078 - val_loss: 0.2062 - val_acc: 0.9315\n",
      "Epoch 18/500\n",
      "12059/12059 [==============================] - 1s 113us/step - loss: 0.2660 - acc: 0.9130 - val_loss: 0.2127 - val_acc: 0.9301\n",
      "Epoch 19/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.2669 - acc: 0.9142 - val_loss: 0.2013 - val_acc: 0.9359\n",
      "Epoch 20/500\n",
      "12059/12059 [==============================] - 1s 110us/step - loss: 0.2614 - acc: 0.9173 - val_loss: 0.1970 - val_acc: 0.9377\n",
      "Epoch 21/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.2602 - acc: 0.9160 - val_loss: 0.2027 - val_acc: 0.9352\n",
      "Epoch 22/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.2533 - acc: 0.9176 - val_loss: 0.1916 - val_acc: 0.9384\n",
      "Epoch 23/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.2450 - acc: 0.9190 - val_loss: 0.1916 - val_acc: 0.9367\n",
      "Epoch 24/500\n",
      "12059/12059 [==============================] - 1s 117us/step - loss: 0.2437 - acc: 0.9174 - val_loss: 0.1846 - val_acc: 0.9402\n",
      "Epoch 25/500\n",
      "12059/12059 [==============================] - 1s 119us/step - loss: 0.2421 - acc: 0.9196 - val_loss: 0.1792 - val_acc: 0.9419\n",
      "Epoch 26/500\n",
      "12059/12059 [==============================] - 1s 116us/step - loss: 0.2409 - acc: 0.9202 - val_loss: 0.1810 - val_acc: 0.9426\n",
      "Epoch 27/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.2368 - acc: 0.9204 - val_loss: 0.1790 - val_acc: 0.9428\n",
      "Epoch 28/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.2287 - acc: 0.9250 - val_loss: 0.1772 - val_acc: 0.9434\n",
      "Epoch 29/500\n",
      "12059/12059 [==============================] - 2s 126us/step - loss: 0.2326 - acc: 0.9263 - val_loss: 0.1726 - val_acc: 0.9460\n",
      "Epoch 30/500\n",
      "12059/12059 [==============================] - 2s 130us/step - loss: 0.2245 - acc: 0.9262 - val_loss: 0.1737 - val_acc: 0.9439\n",
      "Epoch 31/500\n",
      "12059/12059 [==============================] - 1s 112us/step - loss: 0.2217 - acc: 0.9261 - val_loss: 0.1681 - val_acc: 0.9436\n",
      "Epoch 32/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.2188 - acc: 0.9306 - val_loss: 0.1739 - val_acc: 0.9453\n",
      "Epoch 33/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.2124 - acc: 0.9304 - val_loss: 0.1639 - val_acc: 0.9461\n",
      "Epoch 34/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.2077 - acc: 0.9308 - val_loss: 0.1692 - val_acc: 0.9456\n",
      "Epoch 35/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.2137 - acc: 0.9268 - val_loss: 0.1652 - val_acc: 0.9461\n",
      "Epoch 36/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.2103 - acc: 0.9301 - val_loss: 0.1681 - val_acc: 0.9441\n",
      "Epoch 37/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.2113 - acc: 0.9317 - val_loss: 0.1614 - val_acc: 0.9497\n",
      "Epoch 38/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.1990 - acc: 0.9317 - val_loss: 0.1679 - val_acc: 0.9490\n",
      "Epoch 39/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.2073 - acc: 0.9338 - val_loss: 0.1630 - val_acc: 0.9488\n",
      "Epoch 40/500\n",
      "12059/12059 [==============================] - 1s 103us/step - loss: 0.2005 - acc: 0.9323 - val_loss: 0.1620 - val_acc: 0.9471\n",
      "Epoch 41/500\n",
      "12059/12059 [==============================] - 1s 110us/step - loss: 0.1952 - acc: 0.9361 - val_loss: 0.1590 - val_acc: 0.9498\n",
      "Epoch 42/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1906 - acc: 0.9353 - val_loss: 0.1592 - val_acc: 0.9495\n",
      "Epoch 43/500\n",
      "12059/12059 [==============================] - 1s 103us/step - loss: 0.1942 - acc: 0.9335 - val_loss: 0.1555 - val_acc: 0.9492\n",
      "Epoch 44/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.1891 - acc: 0.9372 - val_loss: 0.1521 - val_acc: 0.9510\n",
      "Epoch 45/500\n",
      "12059/12059 [==============================] - 1s 105us/step - loss: 0.1896 - acc: 0.9355 - val_loss: 0.1518 - val_acc: 0.9503\n",
      "Epoch 46/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.1932 - acc: 0.9355 - val_loss: 0.1545 - val_acc: 0.9510\n",
      "Epoch 47/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1901 - acc: 0.9373 - val_loss: 0.1537 - val_acc: 0.9500\n",
      "Epoch 48/500\n",
      "12059/12059 [==============================] - 1s 105us/step - loss: 0.1888 - acc: 0.9367 - val_loss: 0.1516 - val_acc: 0.9514\n",
      "Epoch 49/500\n",
      "12059/12059 [==============================] - 2s 149us/step - loss: 0.1863 - acc: 0.9378 - val_loss: 0.1483 - val_acc: 0.9534\n",
      "Epoch 50/500\n",
      "12059/12059 [==============================] - 1s 103us/step - loss: 0.1807 - acc: 0.9395 - val_loss: 0.1490 - val_acc: 0.9517\n",
      "Epoch 51/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.1809 - acc: 0.9414 - val_loss: 0.1505 - val_acc: 0.9514\n",
      "Epoch 52/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.1768 - acc: 0.9392 - val_loss: 0.1522 - val_acc: 0.9537\n",
      "Epoch 53/500\n",
      "12059/12059 [==============================] - 1s 115us/step - loss: 0.1731 - acc: 0.9424 - val_loss: 0.1548 - val_acc: 0.9502\n",
      "Epoch 54/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.1702 - acc: 0.9408 - val_loss: 0.1465 - val_acc: 0.9529\n",
      "Epoch 55/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.1773 - acc: 0.9427 - val_loss: 0.1587 - val_acc: 0.9497\n",
      "Epoch 56/500\n",
      "12059/12059 [==============================] - 2s 132us/step - loss: 0.1736 - acc: 0.9448 - val_loss: 0.1462 - val_acc: 0.9534\n",
      "Epoch 57/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.1740 - acc: 0.9414 - val_loss: 0.1440 - val_acc: 0.9537\n",
      "Epoch 58/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1712 - acc: 0.9424 - val_loss: 0.1461 - val_acc: 0.9535\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1724 - acc: 0.9444 - val_loss: 0.1470 - val_acc: 0.9544\n",
      "Epoch 60/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.1667 - acc: 0.9438 - val_loss: 0.1436 - val_acc: 0.9554\n",
      "Epoch 61/500\n",
      "12059/12059 [==============================] - 1s 104us/step - loss: 0.1746 - acc: 0.9400 - val_loss: 0.1417 - val_acc: 0.9562\n",
      "Epoch 62/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.1680 - acc: 0.9450 - val_loss: 0.1509 - val_acc: 0.9515\n",
      "Epoch 63/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.1651 - acc: 0.9444 - val_loss: 0.1441 - val_acc: 0.9539\n",
      "Epoch 64/500\n",
      "12059/12059 [==============================] - 1s 107us/step - loss: 0.1645 - acc: 0.9437 - val_loss: 0.1385 - val_acc: 0.9562\n",
      "Epoch 65/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1566 - acc: 0.9450 - val_loss: 0.1460 - val_acc: 0.9544\n",
      "Epoch 66/500\n",
      "12059/12059 [==============================] - 2s 126us/step - loss: 0.1615 - acc: 0.9457 - val_loss: 0.1405 - val_acc: 0.9552\n",
      "Epoch 67/500\n",
      "12059/12059 [==============================] - 1s 123us/step - loss: 0.1549 - acc: 0.9474 - val_loss: 0.1390 - val_acc: 0.9559\n",
      "Epoch 68/500\n",
      "12059/12059 [==============================] - 1s 115us/step - loss: 0.1510 - acc: 0.9495 - val_loss: 0.1422 - val_acc: 0.9552\n",
      "Epoch 69/500\n",
      "12059/12059 [==============================] - 2s 138us/step - loss: 0.1544 - acc: 0.9480 - val_loss: 0.1417 - val_acc: 0.9561\n",
      "Epoch 70/500\n",
      "12059/12059 [==============================] - 1s 121us/step - loss: 0.1568 - acc: 0.9458 - val_loss: 0.1377 - val_acc: 0.9574\n",
      "Epoch 71/500\n",
      "12059/12059 [==============================] - 1s 118us/step - loss: 0.1505 - acc: 0.9465 - val_loss: 0.1392 - val_acc: 0.9556\n",
      "Epoch 72/500\n",
      "12059/12059 [==============================] - 1s 115us/step - loss: 0.1549 - acc: 0.9473 - val_loss: 0.1353 - val_acc: 0.9574\n",
      "Epoch 73/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1446 - acc: 0.9509 - val_loss: 0.1380 - val_acc: 0.9581\n",
      "Epoch 74/500\n",
      "12059/12059 [==============================] - 1s 112us/step - loss: 0.1514 - acc: 0.9486 - val_loss: 0.1385 - val_acc: 0.9542\n",
      "Epoch 75/500\n",
      "12059/12059 [==============================] - 1s 114us/step - loss: 0.1483 - acc: 0.9510 - val_loss: 0.1353 - val_acc: 0.9572\n",
      "Epoch 76/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1381 - acc: 0.9523 - val_loss: 0.1325 - val_acc: 0.9567\n",
      "Epoch 77/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1465 - acc: 0.9482 - val_loss: 0.1340 - val_acc: 0.9559\n",
      "Epoch 78/500\n",
      "12059/12059 [==============================] - 1s 110us/step - loss: 0.1433 - acc: 0.9515 - val_loss: 0.1368 - val_acc: 0.9567\n",
      "Epoch 79/500\n",
      "12059/12059 [==============================] - 2s 127us/step - loss: 0.1427 - acc: 0.9514 - val_loss: 0.1380 - val_acc: 0.9576\n",
      "Epoch 80/500\n",
      "12059/12059 [==============================] - 2s 131us/step - loss: 0.1340 - acc: 0.9544 - val_loss: 0.1317 - val_acc: 0.9589\n",
      "Epoch 81/500\n",
      "12059/12059 [==============================] - 2s 136us/step - loss: 0.1394 - acc: 0.9539 - val_loss: 0.1357 - val_acc: 0.9576\n",
      "Epoch 82/500\n",
      "12059/12059 [==============================] - 1s 117us/step - loss: 0.1393 - acc: 0.9522 - val_loss: 0.1428 - val_acc: 0.9540\n",
      "Epoch 83/500\n",
      "12059/12059 [==============================] - 2s 125us/step - loss: 0.1393 - acc: 0.9539 - val_loss: 0.1311 - val_acc: 0.9584\n",
      "Epoch 84/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1400 - acc: 0.9531 - val_loss: 0.1290 - val_acc: 0.9593\n",
      "Epoch 85/500\n",
      "12059/12059 [==============================] - 2s 138us/step - loss: 0.1350 - acc: 0.9531 - val_loss: 0.1355 - val_acc: 0.9576\n",
      "Epoch 86/500\n",
      "12059/12059 [==============================] - 2s 134us/step - loss: 0.1340 - acc: 0.9540 - val_loss: 0.1310 - val_acc: 0.9599\n",
      "Epoch 87/500\n",
      "12059/12059 [==============================] - 2s 150us/step - loss: 0.1357 - acc: 0.9547 - val_loss: 0.1288 - val_acc: 0.9604\n",
      "Epoch 88/500\n",
      "12059/12059 [==============================] - 1s 115us/step - loss: 0.1355 - acc: 0.9542 - val_loss: 0.1300 - val_acc: 0.9584\n",
      "Epoch 89/500\n",
      "12059/12059 [==============================] - 2s 130us/step - loss: 0.1330 - acc: 0.9536 - val_loss: 0.1332 - val_acc: 0.9591\n",
      "Epoch 90/500\n",
      "12059/12059 [==============================] - 2s 147us/step - loss: 0.1319 - acc: 0.9546 - val_loss: 0.1317 - val_acc: 0.9583\n",
      "Epoch 91/500\n",
      "12059/12059 [==============================] - 1s 105us/step - loss: 0.1275 - acc: 0.9562 - val_loss: 0.1287 - val_acc: 0.9588\n",
      "Epoch 92/500\n",
      "12059/12059 [==============================] - 1s 113us/step - loss: 0.1256 - acc: 0.9572 - val_loss: 0.1300 - val_acc: 0.9596\n",
      "Epoch 93/500\n",
      "12059/12059 [==============================] - 2s 133us/step - loss: 0.1283 - acc: 0.9561 - val_loss: 0.1284 - val_acc: 0.9589\n",
      "Epoch 94/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1290 - acc: 0.9551 - val_loss: 0.1328 - val_acc: 0.9598\n",
      "Epoch 95/500\n",
      "12059/12059 [==============================] - 1s 114us/step - loss: 0.1307 - acc: 0.9557 - val_loss: 0.1249 - val_acc: 0.9604\n",
      "Epoch 96/500\n",
      "12059/12059 [==============================] - 1s 108us/step - loss: 0.1299 - acc: 0.9578 - val_loss: 0.1268 - val_acc: 0.9593\n",
      "Epoch 97/500\n",
      "12059/12059 [==============================] - 1s 110us/step - loss: 0.1171 - acc: 0.9588 - val_loss: 0.1237 - val_acc: 0.9623\n",
      "Epoch 98/500\n",
      "12059/12059 [==============================] - 1s 116us/step - loss: 0.1208 - acc: 0.9586 - val_loss: 0.1340 - val_acc: 0.9588\n",
      "Epoch 99/500\n",
      "12059/12059 [==============================] - 1s 114us/step - loss: 0.1194 - acc: 0.9591 - val_loss: 0.1343 - val_acc: 0.9591\n",
      "Epoch 100/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1233 - acc: 0.9580 - val_loss: 0.1252 - val_acc: 0.9621\n",
      "Epoch 101/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1180 - acc: 0.9599 - val_loss: 0.1294 - val_acc: 0.9598\n",
      "Epoch 102/500\n",
      "12059/12059 [==============================] - 1s 112us/step - loss: 0.1184 - acc: 0.9590 - val_loss: 0.1329 - val_acc: 0.9584\n",
      "Epoch 103/500\n",
      "12059/12059 [==============================] - ETA: 0s - loss: 0.1190 - acc: 0.959 - 1s 112us/step - loss: 0.1190 - acc: 0.9593 - val_loss: 0.1253 - val_acc: 0.9613\n",
      "Epoch 104/500\n",
      "12059/12059 [==============================] - 1s 117us/step - loss: 0.1168 - acc: 0.9578 - val_loss: 0.1281 - val_acc: 0.9615\n",
      "Epoch 105/500\n",
      "12059/12059 [==============================] - 1s 109us/step - loss: 0.1173 - acc: 0.9609 - val_loss: 0.1239 - val_acc: 0.9611\n",
      "Epoch 106/500\n",
      "12059/12059 [==============================] - 1s 117us/step - loss: 0.1149 - acc: 0.9586 - val_loss: 0.1235 - val_acc: 0.9621\n",
      "Epoch 107/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1140 - acc: 0.9599 - val_loss: 0.1223 - val_acc: 0.9611\n",
      "Epoch 108/500\n",
      "12059/12059 [==============================] - 1s 111us/step - loss: 0.1123 - acc: 0.9643 - val_loss: 0.1232 - val_acc: 0.9611\n",
      "Epoch 109/500\n",
      "12059/12059 [==============================] - 1s 115us/step - loss: 0.1089 - acc: 0.9618 - val_loss: 0.1205 - val_acc: 0.9603\n",
      "Epoch 110/500\n",
      "12059/12059 [==============================] - 1s 119us/step - loss: 0.1073 - acc: 0.9633 - val_loss: 0.1243 - val_acc: 0.9620\n",
      "Epoch 111/500\n",
      "12059/12059 [==============================] - 1s 120us/step - loss: 0.1107 - acc: 0.9633 - val_loss: 0.1175 - val_acc: 0.9631\n",
      "Epoch 112/500\n",
      "12059/12059 [==============================] - 1s 106us/step - loss: 0.1056 - acc: 0.9638 - val_loss: 0.1220 - val_acc: 0.9618\n",
      "Epoch 113/500\n",
      "12059/12059 [==============================] - 1s 116us/step - loss: 0.1122 - acc: 0.9614 - val_loss: 0.1215 - val_acc: 0.9618\n",
      "Epoch 114/500\n",
      "12059/12059 [==============================] - 1s 112us/step - loss: 0.1070 - acc: 0.9650 - val_loss: 0.1216 - val_acc: 0.9618\n",
      "Epoch 115/500\n",
      "12059/12059 [==============================] - 1s 118us/step - loss: 0.1075 - acc: 0.9629 - val_loss: 0.1211 - val_acc: 0.9606\n",
      "Epoch 116/500\n",
      "12059/12059 [==============================] - 2s 126us/step - loss: 0.1047 - acc: 0.9634 - val_loss: 0.1181 - val_acc: 0.9636\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6912/12059 [================>.............] - ETA: 0s - loss: 0.1040 - acc: 0.9647"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_split=.33,\n",
    "          batch_size=batch_size,\n",
    "#           callbacks=callbacks_list, verbose=1,\n",
    "#           validation_data=(x_test, y_test),\n",
    "          epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
